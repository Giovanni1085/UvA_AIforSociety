{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration Networks in the US Congress\n",
    "\n",
    "In this project, we will work through a real-world example looking at collaboration networks in the US Congress. It is typical in terms of the data challenges you face with social data as well as the kind of new questions you can ask.\n",
    "\n",
    "Legislatures are inherently difficult to investigate. Political scientists have been interested in collaborations among parliamentarians as explanatory factors in legislative behavior for decades. But up to now they have lacked the data to do this on a larger scale. Recently, legislatures around the world have begun to to publish all their workings on the web. Especially advanced is the US Congress. Take a look at  https://www.congress.gov/. \n",
    "\n",
    "In this exercise, we will first download (web-scrape) a sample of US Congress bills and then investigate sponsors and cosponsors of these bills. If you do not know much about the workings of the US congress, take a look at https://en.wikipedia.org/wiki/Sponsor_(legislative) \n",
    "\n",
    "First, we load a couple of libraries that we need. Some you will recognise. Hopefully, all the libraries should have been installed for you beforehand if you use a framework like Anaconda (https://www.anaconda.com/). But if at any moment in time Python does not recognise the library, you can simply install it by following the online instructions.\n",
    "\n",
    "<style>\n",
    "    table {\n",
    "        display: inline-block\n",
    "    }\n",
    "</style>\n",
    "\n",
    "Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep cell\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import random\n",
    "import json\n",
    "\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the congress number we are interested in. Because we talked about congress 114 in another exercise, I define congress_no as the 114th congress but you can change it of course. Just make sure that the congress actually exists.\n",
    "\n",
    "Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep cell\n",
    "\n",
    "congress_no = 114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is to download a random sample of bill information for this congress. The first challenge is to find out how many legilsations they were in this congress. \n",
    "\n",
    "We have defined a URL_all for you where you can find the information with some web parsing. Looking at the URL how does this work?\n",
    "\n",
    "Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep cell\n",
    "\n",
    "URL_all = 'https://www.congress.gov/search?q={%22source%22:%22legislation%22,%22congress%22:' + str(congress_no) + ',%22type%22:%22bills%22,%22chamber%22:%22House%22}&searchResultViewType=expanded'\n",
    "URL_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply your web-parsing skills to find the number of legislations and assign the result to `max_legislation`. Finally, print out `max_legislation`. \n",
    "\n",
    "Tip: You should see legislation and 114 congress ticked in the left-hand menu following URL_all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get 6536. Hopefully, you too?\n",
    "\n",
    "Create a sample of bills you would like to download. max_n is that number. 10 is a good sample to keep our requests to the server low.\n",
    "\n",
    "Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep cell\n",
    "\n",
    "max_n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `random_bills` as a random list of integers with `max_n` bill numbers for download. \n",
    "\n",
    "We do not care much at this moment whether the bill actually exists. When we download the bills we will check and we are happy to skip a couple because we are interested here in a sample anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `random_bill` texts into a list called `pages`. \n",
    "\n",
    "Make sure that the bill page exists in case you happen to have selected a bill number that does not (status_code = 200). Also, I advise you to capture any other exception that might occur. You don't want to be at the last bill for the whole process breaks down. As said, we are not really interested in any particular bill but a sample.\n",
    "\n",
    "After accessing each bill, I recommend to put Python to sleep for a little bit in order for the congress web server to have a chance to recover. Once you have run this, do not repeat this too often so that you don't overload the server.\n",
    "\n",
    "The information you are looking for is in the 'all-info' pages for a bill. So, e.g., https://www.congress.gov/bill/114th-congress/house-bill/5245/all-info. Here, you can find later on the sponsor and co-sponsor information. You will want to use the requests library to download the all-info pages.\n",
    "\n",
    "Tip: This is the moment a progress bar in your for loop is very handy. Check out https://github.com/tqdm/tqdm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly used library in Python to parse websites is BeautifulSoup. Check out https://realpython.com/python-web-scraping-practical-introduction/.\n",
    "\n",
    "BeautifulSoup generally takes in a page from requests and applies an HTML parser. We have already loaded its class from the bs4 library above.\n",
    "\n",
    "Next define a function `extract_data_website(soup, bill_no)` that takes in the result of BeautifulSoup parsing of a website called soup, a bill number and for that bill returns a data frame with the bill number, its sponsors and its co-sponsors like `df_temp[['Bill', 'Sponsor', 'Cosponsor']]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next run `extract_data_website` against all pages in the `pages` list. Store the results in a data frame called `sponsors_bill_df` with three columns 'Bill' for the bill number, 'Sponsor' which contains the name of the sponsor, and 'Cosponsor', which contains the name of a co-sponsor if there is one and otherwise nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the first 20 rows of sponsors_bill_df. The rows should look like:\n",
    "\n",
    "\n",
    "\n",
    "| Bill | Sponsor | Cosponsor |\n",
    "| --- | --- | --- |\n",
    "| Bill-9998 | Rep. Smith, Jane   |  |\n",
    "| Bill-9999 | Rep. Smith, Jane | Rep. Smith, John |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to tidy the data a bit following the famous tidy data principles by Wickham: https://www.jeannicholashould.com/tidy-data-in-python.html. This means we want that \n",
    "- each variable forms a column and contains values.\n",
    "- each observation forms a row.\n",
    "- each type of observational unit forms a table.\n",
    "\n",
    "This is easy to do in Pandas with the merge command. Create a new data frame called `sponsors_cosponsors_tidy` with three columns: The first is 'Bill' for the bill number, the second 'Sponsor-Cosponsor', which contains either 'Sponsor' or 'Cosponsor', and the third column is 'Name' for the name of the representative. For example:\n",
    "\n",
    "| Bill | Sponsor-Cosponsor | Name |\n",
    "| :-  | :-  | :-  |\n",
    "| Bill-9999 | Sponsor   | Rep. Smith, Jane |\n",
    "| Bill-9999 | Cosponsor | Rep. Smith, John |\n",
    "\n",
    "Tip: You can create `sponsors_cosponsors_tidy` in one line. But of course, you can also do it with more lines if you prefer - with a loop, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data frame is nice but for a numerical analysis like clustering we often need a matrix. Create `sponsors_cosponsors_matrix` from `sponsors_cosponsors_tidy`, which has the bill numbers as rows and the representative names as columns. The matrix should contain 'Sponsor' for the sponsor of a bill and 'Cosponsor' for the cosponsors. Otherwise, all cells should have NaN values. Again, you only need one line or use a loop if you want.\n",
    "\n",
    "Tip: In the previous cell you created a long format. Now you want to make it a wide format. Check Google ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the first couple of rows of this very sparse matrix. It should look like (but much much bigger):\n",
    "\n",
    "| Name Bill | Rep. Smith, Jane | Rep. Smith, John | Rep. Smith, Mary|\n",
    "| :-  | :-  | :-  | :- |\n",
    "| Bill-9997 | NaN   | Sponsor | NaN |\n",
    "| Bill-9999 | Cosponsor | NaN | NaN |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this matrix describes a community of sponsors and cosponsors, we will try our clustering approach next. We try k-means on clusters of sponsors and cosponsors.\n",
    "\n",
    "## Kmeans\n",
    "\n",
    "First we load the KMeans library and define k = 10. Run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep cell\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 10                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some data preparation next. Define a matrix kmeans_matrix that is 1 where sponsors_cosponsors_matrix contains a sponsor or cosponsor and otherwise 0. A loop might be useful or check. There is, however, also a way to do this in one line ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell for the k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep cell\n",
    "\n",
    "kmeans_clusters = KMeans(n_clusters  = k) \n",
    "kmeans_clusters.fit(kmeans_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the cluster centres for all members. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something intereting might be hidden in all this but overall the matrix does not have enough entries to deliver any meaningful conclusions from clustering.\n",
    "\n",
    "## Association Rules\n",
    "\n",
    "This is why we would like to try another approach. We will try to learn rules that represent the data. We imagine that it should be possible this way to determine rules such as: If senator x proposes a bill, congresswoman y will be involved, too, as she is associated with her. \n",
    "\n",
    "Association rules are often used for 'market basket analysis', which are typical example for analysing very sparse datasets. The result of a market basket analysis is a set of association rules that specify patterns of relationships between items. A typical rule might be expressed in the form: \n",
    "\n",
    "`{peanut butter, jelly} -> {bread}`.\n",
    "\n",
    "So, if a costumer buys peanut butter and jelly, she will also get bread. We will try to learn something similar by associating US congress members who sponsor bills together. If congresswoman X sponsors a bill, so will senator Y.\n",
    "\n",
    "Perhaps the most-widely used approach for efficiently analysing large data for rules is known as Apriori. This algorithm was introduced in 1994 by R. Agrawal and R. Srikant, and has become somewhat synonymous with association rule learning since then. Check out: Fast algorithms for mining association rule, in Proceedings of the 20th International Conference on Very Large Databases, pp. 487-499, by R. Agrawal, and R.Srikant, (1994).\n",
    "\n",
    "However, I found that the apriori algorithm (https://pbpython.com/market-basket-analysis.html) is extremely slow in Python. Rather than Apriori, we will use a faster version (http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpgrowth/) that implements Han, Jiawei, Jian Pei, Yiwen Yin, and Runying Mao. \"Mining frequent patterns without candidate generation. \"A frequent-pattern tree approach.\" Data mining and knowledge discovery 8, no. 1 (2004): 53-87. It concentrates on frequent itemsets: \"FP-Growth is an algorithm for extracting frequent itemsets with applications in association rule learning that emerged as a popular alternative to the established Apriori algorighm. In general, the algorithm has been designed to operate on databases containing transactions, such as purchases by customers of a store. An itemset is considered as 'frequent' if it meets a user-specified support threshold. For instance, if the support threshold is set to 0.5 (50%), a frequent itemset is defined as a set of items that occur together in at least 50% of all transactions in the database.\"\n",
    "\n",
    "We follow the examples given at http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpgrowth/ including the TransactionEncoder. But we could have actually reduced the steps, as we have already have sponsors_cosponsors_matrix. \n",
    "\n",
    "Load the Python version of FP-Growth by running the cell below. You might have to install mlxtend first, a library of data science tools. See http://rasbt.github.io/mlxtend/installation/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep cell\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fpgrowth library requires our dataset to be in the form of a list of lists, where the whole dataset is a big list and each transaction in the dataset is an inner list within the outer big list. Currently, we have data in the form of a pandas data frame. \n",
    "\n",
    "Convert sponsors_cosponsors_tidy into such a list of list called records. The inner list contains for each bill the list of sponsors and cosponsors. The outer list is simply equivalent to the number of bills. \n",
    "\n",
    "Make sure that the inner lists do not contain duplicates.\n",
    "\n",
    "It should look like:\n",
    "```\n",
    "[['Rep. Smith, Jane', 'Rep. Schweikert, David'],\n",
    " ['Rep. Buchanan, Vern', 'Rep. Smith, John', ...],\n",
    " ...\n",
    " ]\n",
    "\n",
    "```\n",
    "Tip: Google for 'Pandas group_by and list' ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpgrowth/, to transfrom records into the right format for fpgrowth via the TransactionEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see what we could have done directly with sponsors_cosponsors_matrix in order to arrive at the same output? All the association mining/clustering algorithms have similar inputs.\n",
    "\n",
    "Let us finally return the records with at least 20% support. 20% is not much of course, but we also limited ourselves to only a small subset of bills. Use the `fpgrowth` function.\n",
    "\n",
    "Tip: Make sure to set use_colnames=True to get a readable output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might not get great results, as the dataset is small. If you want you can download more bills and play with the support. \n",
    "\n",
    "Or you could now start to look up the sponsors/co-sponsors on wikipedia and find out whether there are, e.g., bi-partisan bills?\n",
    "\n",
    "## Network\n",
    "\n",
    "We have learned a lot today about how to analyse communties with algorithms. While k-means was less convincing because there was too little data, the association rules migth show a few interesting results at least?\n",
    "\n",
    "Most people would look at another way of working through this data with so-called social network analysis. Check out https://en.wikipedia.org/wiki/Social_network_analysis. \n",
    "\n",
    "In Python we need the networkx libraries. Install it if you haven't got it: https://networkx.org/documentation/stable/install.html.\n",
    "\n",
    "Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep cell\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to create a social network graph G from sponsors_bill_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep cell\n",
    "\n",
    "G = nx.from_pandas_edgelist(sponsors_bill_df, \"Sponsor\", \"Cosponsor\", [\"Bill\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try and visualise the network G. Check the networkx documentation if you need to. \n",
    "\n",
    "There are a lot of online tutorials how to use networkx. The hard part will be plot all the nodes and edges in the graph so that they make sense together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can make the network as informative and complicated as you want ... Let's move on for now to a more hands-on analysis.\n",
    "\n",
    "## Biographies\n",
    "\n",
    "After all this association mining with mixed results, you might decide to try a different approach and look at the US Congress bios for further information. Maybe, you want to know more about the biographical background of members of congress or mine their Twitter feeds. \n",
    "\n",
    "We can access the propublica congress API for that. It is explained at https://www.propublica.org/datastore/api/propublica-congress-api.\n",
    "\n",
    "First define the correct url by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep cell\n",
    "\n",
    "propublica_url = 'https://api.propublica.org/congress/v1/' + str(congress_no) + '/senate/members.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now request an API key from https://www.propublica.org/datastore/api/propublica-congress-api and use it to set 'X-API-Key' in a 'headers' dictionary for requests. Something like\n",
    "`headers = {'X-API-Key': 'a1b2c3d4efghijklmnop-my-key'}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the data into a varibale `res` and print out the first results in number of characters. I used 3000 characters, but feel free to print out as many as you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a bit difficult to read but you hopefully recognise this as a JSON response, which is very typical for API calls. \n",
    "\n",
    "You can read the results into a dictionary with the json package that we imported earlier.\n",
    "\n",
    "Load this dictionary into a a data frame called `members_df` and print out the first couple of rows. It's very easy to do with the right pandas function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the column names of `members_df`. It should look like:\n",
    "```\n",
    "Index(['id', 'title', 'short_title', 'api_uri', 'first_name', 'middle_name',\n",
    "       'last_name', 'suffix', 'date_of_birth', 'gender', 'party',\n",
    "       'leadership_role', 'twitter_account', 'facebook_account',\n",
    "       'youtube_account', 'govtrack_id', 'cspan_id', 'votesmart_id',\n",
    "       'icpsr_id', 'crp_id', 'google_entity_id', 'fec_candidate_id', 'url',\n",
    "       'rss_url', 'contact_form', 'in_office', 'cook_pvi', 'dw_nominate',\n",
    "       'ideal_point', 'seniority', 'next_election', 'total_votes',\n",
    "       'missed_votes', 'total_present', 'last_updated', 'ocd_id', 'office',\n",
    "       'phone', 'fax', 'state', 'senate_class', 'state_rank', 'lis_id',\n",
    "       'missed_votes_pct', 'votes_with_party_pct', 'votes_against_party_pct'],\n",
    "      dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `members_df` and the column names, create two functions: \n",
    "\n",
    "The first function `get_representative_bio(congress_no)` takes in a congress number and returns a data frame of people in the house of representatives that shows the following information for them: ['first_name', 'last_name', 'date_of_birth', 'gender', 'party', 'state', 'next_election', 'missed_votes_pct', 'votes_with_party_pct'].\n",
    "\n",
    "The second function `get_senator_bio` does the same but for the senate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representative_bio(congress_no):\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senator_bio(congress_no):\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the first results for `get_senator_bio(114)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you see how rich the information is datasets linked to the US Congress.\n",
    "\n",
    "Association mining and clustering are very powerful techniques to find yet unknown relations. We tried them to detect patterns of maybe unexpected behaviour in the US Congress. \n",
    "\n",
    "In another example, we used them to detect how powerful the big Internet companies are in the mobile ecosystems: https://journals.sagepub.com/doi/full/10.1177/2056305120971632: \"Our research seeks to understand how platforms have been able to technically integrate themselves into the fabric of the mobile ecosystem, transforming the economic dynamics that allow these largely enclosed entities to compete. We therefore want to consider platforms as service assemblages to account for the material ways in which they have decomposed and recomposed themselves for developers, enabling them to shift the economic dynamics of competition and monopolization in their favor. This article will argue that this shift in the formation of platform monopolies is being brought about by the decentralization of these services, leading to an overall technical integration of the largest digital platform such as Facebook and Google into the source code of almost all apps.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
